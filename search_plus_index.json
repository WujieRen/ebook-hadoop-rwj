{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Apache Hadoop软件库是一个框架，该框架允许使用简单的编程模型跨计算机集群对大型数据集进行分布式处理。它旨在从单个服务器扩展到数千台机器，每台机器都提供本地的计算和存储。集群的每台计算机都可能会出现故障，所以该库本身不依赖于硬件来提供高可用性，而是被设计用来检测和处理应用程序层的故障，因此可以在计算机集群的顶部提供高可用性服务。 本文档旨在以简单粗报直接的方式帮助大家使用Hadoop。使用的版本是Hadoop2.7.X。 下载 单击此处下载Apache Hadoop™。 Hadoop的特性 可靠性，高容错 存储的可靠性 HDFS以块（block）的形式来存储数据，Hadoop 1.X版本中块大小是64MB；Hadoop 2.X版本中块的大小为128MB。用户可自定义块大小 HDFS分布式的存储文件块到集群的多台机器上，每台保存一个副本。用户可以自己设置副本数。 对每个存储块文件生成也给校验码，之后定期检测或读取到这个块的时候又会生成一个校验码进行匹配，没匹配上说明文件损坏，Hadoop会自动重新创建一个副本以保证数据的安全性。 计算的可靠性 可扩展性 集群扩容 损坏的硬件重新上架 By renwujie            updated 2021-01-23 23:18:27 "},"01-enviroment/":{"url":"01-enviroment/","title":"01-enviroment","keywords":"","body":"安装部署 单节点模式 Hadoop完全分布式集群 Hadoop HA集群 By renwujie            updated 2021-01-27 23:34:30 "},"01-enviroment/01-cluster-setup.html":{"url":"01-enviroment/01-cluster-setup.html","title":"01-cluster-setup.md","keywords":"","body":"部署完全分布式Hadoop集群 Purpose 本节描述如何搭建一个完全分布式的Hadoop集群，以3台服务器举例。可扩展到若干节点。本文档不涉及安全或高可用性等主题。 Prerequisites 参考：搭建高可用集群时Linux环境必需配置的先决条件，先将Linux基础环境搭建成功。 Installation 点击下载相应版本，本文档采用2.7.7。安装Hadoop集群需要解压所软件在所有的节点上，所以将硬件的功能进行划分很重要。 Component Planning 建议将NameNode和ResourceManager指定分开在两台节点上，这样即使某台节点挂了也不至于导致NameNode和ResoureManager同时挂掉。其余的节点则同时充当DataNode和NodeManager。通常建议将DataNode和ResourceManager安装在同一台节点上。 n1 n2 n3 NameNode/DataNode DataNode DataNode NodeManager NodeManager ResourceManager/NodeManager SecondaryNameNode HistoryServer Configure Hadoop HDFS的守护程序是NameNode，SecondaryNameNode和DataNode。 YARN守护程序是ResourceManager，NodeManager和WebAppProxy。如果要使用MapReduce，则MapReduce作业历史服务器也将运行。对于大型安装，它们通常在单独的主机上运行。 所有配置文件都在路径：$HADOOP_HOME/etc/hadoop 中。 配置JAVA_HOME变量 在hadoop-env.sh、yarn-env.sh、mapred-env.sh中指定JAVA_HOME的路径。 HDFS 配置hdfs文件系统的主机和端口，端口号在hadoop1.x版本默认使用的是9000，而在hadoop2.x中默认使用的是8020。 以及hadoop分布式集群经过格式化后数据保存的位置。 core-site.xml中配置： fs.defaultFS hdfs://n1:8020 hadoop.tmp.dir /opt/cluster/hadoop-2.7.7/data/tmp 在静态web页面上呈现内容时，以参数hadoop.http.staticuser.user指定的用户名作为筛选条件，默认dr.who。 core-site.xml中配置： hadoop.http.staticuser.user renwujie 测试环境可以关闭hdfs中文件访问权限，默认为true。 hdfs-site.xml中配置： dfs.permissions.enabled false YARN 配置ResourceManager的主机名；以及web访问地址。 yarn-site.xml中配置： yarn.resourcemanager.hostname n3 yarn.resourcemanager.webapp.address ${yarn.resourcemanager.hostname}:8088 配置执行MapReduce job的框架。可选值：local，classic，yarn。 mapred-site.xml 中配置： mapreduce.framework.name yarn 如果在mapred-site.xml中设置了mapreduce.framework.name为yarn的话，就需要配置yarn.nodemanager.aux-services参数了。该参数用于指定执行MapReduce job时，yarn使用的shuffle（混淆）技术。如果使用的话值为mapreduce_shuffle。 yarn-site.xml中配置： yarn.nodemanager.aux-services mapreduce_shuffle--> A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers SecondaryNameNode hdfs-site.xml中配置： dfs.namenode.secondary.http-address n1:50090 HistoryServer mapred-site.xml中配置： mapreduce.jobhistory.address n2:10020 mapreduce.jobhistory.webapp.address n2:19888 日志聚合 yarn-site.xml中配置： yarn.log-aggregation-enable true yarn.log-aggregation.retain-seconds 106800 Slaves文件 slaves文件用于指定DataNode和NodeManager所在节点，每行一个。 n1 n2 n3 Operating the Hadoop Cluster 完成以上配置后，将整个$HADOOP_HOME文件分发到集群中的所有节点上。分发前删除掉：$HADOOP_HOME/share/doc目录。因为该目录只是一些参考文档，且很大。分发时将占用大量带宽，浪费时间。 scp -r $HADOOP_HOME root@n2.com.rwj:/opt/ scp -r $HADOOP_HOME root@n3.com.rwj:/opt/ 格式化集群 要启动hadoop集群，需要将HDFS和YARN都启动。 首次启动HDFS时，需要将其格式化。在NameNode机器所在节点上执行： $ HADOOP_HOME/bin/hdfs namenode -format 启动/关闭集群 以下所有命令是启动命令，如果是关闭，将start替换为stop即可。启动时一边启动一边用命令jps查看相应进程是否启动成功。 [info]注意 以下命令单独启动某进程时，必需要在相应的节点上进行该命令的操作。 启动HDFS： # 使用 $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode $HADOOP_HOME/sbin/ hadoop-daemon.sh start datanode # 或者 $HADOOP_HOME/sbin/start-dfs.sh 启动YARN： # 使用 $HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager $HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager # 或者 $HADOOP_HOME/sbin/start-yarn.sh 启动SecondaryNamenode $HADOOP_HOME/sbin hadoop-daemon.sh start secondarynamenode 启动HistoryServer $HADOOP_HOME/bin/mapred --daemon start historyserver #或者 $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver 最终，对应于开始的组件规划（Component Planning）；每台节点上都应该看到相应的进程启动并稳定运行。 Web Interface 成功启动集群后即可查看对应组件的Web UI是否可以正常访问。 Daemon Web Interface Notes NameNode https://n1:50070 默认50070 ResourceManager https://n3:8088 默认8088 MapReduce JobHistory https://n2:19888 默认19888 补充 Hadoop的配置文件 每个模块对应一个配置文件，主要分两种：默认配置文件和自定义配置文件。 | 模块 | 默认配置文件 | 自定义配置文件 | | :-------- | :----------------- | :-------------- | | Core | core-default.xml | core-site.xml | | HDFS | hdfs-default.xml | hdfs-site.xml | | YARN | yarn-default.xml | yarn-site.xml | | MapReduce | mapred-default.xml | mapred-site.xml | 配置文件的优先级：Hadoop的守护进程在启动的时候会首先去读取默认配置文件，再去读取自定义配置文件。自定义配置文件的属性会覆盖掉默认配置文件。 By renwujie            updated 2021-02-06 00:52:46 "},"01-enviroment/02-cluster-ha.html":{"url":"01-enviroment/02-cluster-ha.html","title":"02-cluster-ha.md","keywords":"","body":"部署Hadoop HA集群 Purpose 本节描述如何搭建一个高可用的完全分布式Hadoop集群，以3台服务器举例。 Prerequisites 已经搭建好了一个完全分布式的Hadoop集群。参考上节。 在以上Hadoop的所有节点上安装Zookeeper，配置一个完全分布式的Zookeeper集群。可参考：~~~ Component Planning 在上节的基础上搭建HA Hadoop集群，相当于增加一个有NameNode。这时要用到Zookeeper。机器规划更新如下（加粗的是增加的的）： n1 n2 n3 NameNode/DataNode DataNode NameNode/DataNode NodeManager NodeManager ResourceManager/NodeManager SecondaryNameNode HistoryServer Zookeeper Zookeeper Zookeeper Configure HA Hadoop Cluster 主要修改hdfs-site.xml和core-site.xml两个文件。 如果是刚搭建好的Hadoop分布式集群，还没有运行，可以直接进行HA的配置。如果是已经运行了的集群，建议备份配置文件。 搭建前请关闭所有Hadoop进程和Zookeeper进程。 修改配置文件 hdfs-site.xml 删除SecondaryNameNode的配置——或者一开始配置Hadoop集群是不配置也可以。 给NameNode管理的元数据一个逻辑名称: dfs.nameservices ns 指定两个NameNode的逻辑名称： dfs.ha.namenodes.ns nn1,nn2 指定两个NameNode实例的RPC内部通信地址： dfs.namenode.rpc-address.ns.nn1 n1:8020 dfs.namenode.rpc-address.ns.nn2 n3:8020 指定两个NameNode实例的Http地址： dfs.namenode.http-address.ns.nn1 n1:50070 dfs.namenode.http-address.ns.nn2 n3:50070 指定JournalNode日志节点的URI： dfs.namenode.shared.edits.dir qjournal://n1:8485;n2:8485;n3:8485/ns 指定NournalNode本地存储日志的路径： dfs.journalnode.edits.dir /opt/cluster/hadoop-2.7.7/data/jn 指定代理访问方式： dfs.client.failover.proxy.provider.mycluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider 指定隔离方案： dfs.ha.fencing.methods sshfence dfs.ha.fencing.ssh.private-key-files /home/root/.ssh/id_rsa 开启自动故障转移： dfs.ha.automatic-failover.enabled true core-site.xml 指定Zookeeper的实例和端口号： ha.zookeeper.quorum n1:2181,n2:2181,n3:2181 指定（修改）命名空间： fs.defaultFS hdfs://ns 分发配置文件 如果是一个新的集群，将配置好的Hadoop文件发送到所有节点上。如果是一个已经使用了的集群，只要发送配置文件即可。 同步NameNode节点元数据 & 启动HA Hadoop集群 同步NameNode节点的元数据要严格按照以下顺序操作。 启动所有节点的Zookeeper； zkServer.sh start 启动所有的JournalNode； $HADOOP_HOME/sbin/hadoop-daemon.sh start journalnode 同步元数据； 如果是一个新集群：格式化（只需要格式化一台NameNode即可）。否则不需要格式化。 如果不是一个新集群，就直接在旧的NamoNode节点上，初始化JournalNode（注意初始化JournalNode前要先启动所有的JournalNode）。初始化命令如下： $HADOOP_HOME/bin/hdfs namenode -initializeSharedEdits 之后，启动NameNode。 然后到另一台NameNode节点上同步JournalNode。同步命令如下： $HADOOP_HOME/bin/hdfs namenode -bootstrapStandby 这时候如果把第二台NameNode也启动，那么在WebUI就能看到两个NameNode节点都处于StandBy的状态。 元数据同步完成后，先关闭所有NameNode进程。然后在任意NameNode所在节点上初始化ZKFC。命令如下： $HADOOP_HOME/bin/hdfs zkfc -formatZK 初始化ZKFC完成后，进入zk客户端，检查是否生成了名为hadoop-ha的节点目录： zkCli.sh start ls / 在两台NameNode所在的节点上分别启动zkfc监听器： $HADOOP_HOME/sbin/hadoop-daemon.sh start zkfc 进程对应的名称为：DFSZKFailoverController。可用jps查看是否启动成功。 启动HDFS和YARN服务。 这时在启动两个NameNode后就会发现一个NameNode状态为active，另一个为standby。 初次启动HA需要按照以上步骤同步元数据。后续启动HA集群，集群就会自动进行元数据同步监听和故障容错，只要按顺序启动相应组件即可。 补充 后续重新启动集群时也严格按照：ZK -> JournalNode -> ZKFC -> HDFS -> YARN 的顺序启动。 关闭顺序建议：YARN -> HDFS -> ZKFC -> JournalNode -> ZK。 如果手动启动麻烦，只需要调整start-dfs.sh中NameNodes、DataNodes、JournalNode和ZKFC启动的顺序即可调用start-dfs.sh和start-yarn.sh启动。更进一步，可以自己写一个shell脚本，一个脚本按顺序启动、关闭所有进程。 By renwujie            updated 2021-02-07 22:09:42 "},"02-basic-operate/":{"url":"02-basic-operate/","title":"02-basic-operate","keywords":"","body":"基本操作 By renwujie            updated 2021-01-27 23:22:29 "},"03-code-demo/":{"url":"03-code-demo/","title":"03-code-demo","keywords":"","body":"案例 代码案例在Windows环境中开发，需要WIndows本地做一些配置。参考： Windows本地开发Hadoop代码环境配置。 Hadoop编程主要时编写MapReduce任务。MapReduce任务模板参考： MapReduce 模板代码 案例参考： WordCount案例 MapReduce计数器使用 MapReduce Combine案例 二次排序案例 MapReduce实现join功能案例 以上所有案例可参考： By renwujie            updated 2021-01-28 14:52:32 "},"03-code-demo/01-dev-env/":{"url":"03-code-demo/01-dev-env/","title":"01-dev-env","keywords":"","body":"Windows本地开发Hadoop代码环境配置 安装好jdk、maven、idea等基础组件，具备Windows本地开发Java代码的环境； 下载Hadoop，并解压到本地； 下载相应版本的winutils.exe文件和hadoop.dll文件，放到Windows本地的$HADOOP_HOME/bin目录下； 如果要把数据上传到HDFS上，在Windows本地和HDFS远程交互的话，需要在resource目录下添加我们安装好的集群中的：core-site.xml和hdfs-site.xml，两个配置文件；如果数据只是在本地，全程开发和测试都只是在本地，不和HDFS进行数据交互的话，就不需要这两个文件，否则会报异常（找不到路径）； 创建Maven项目，在pom.xml中添加开发Hadoop代码所需依赖包。 org.apache.hadoop hadoop-client ${hadoop.version} 配置好以后的目录可参考： By renwujie            updated 2021-02-04 20:52:02 "},"03-code-demo/02-mr-model/":{"url":"03-code-demo/02-mr-model/","title":"02-mr-model","keywords":"","body":"MapReduce编程模板 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import java.io.IOException; /** * Hello world! */ public class MRModule extends Configured implements Tool { public static class MrMapper extends Mapper { @Override protected void setup(Context context) throws IOException, InterruptedException { super.setup(context); } @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { } @Override protected void cleanup(Context context) throws IOException, InterruptedException { super.cleanup(context); } } //Reduce public static class MrReducer extends Reducer { @Override public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { } } //Driver public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException { //创建Job // Configuration conf = new Configuration(); Configuration conf = this.getConf(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); //设置Job Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(MrMapper.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setReducerClass(MrReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //提交Job boolean status = job.waitForCompletion(true); return status ? 0 : 1; } public static void main(String[] args) throws Exception { // int status = new MRModule().run(args); Configuration conf = new Configuration(); int status = ToolRunner.run(conf, new MRModule(), args); System.exit(status); } } By renwujie            updated 2021-01-28 21:38:37 "},"03-code-demo/03-wd-count/":{"url":"03-code-demo/03-wd-count/","title":"03-wd-count","keywords":"","body":"WordCount案例 WordCount的流程是这样的：针对输入的文件，按照指定的分隔符将字段分割，并形成形式的键值对。Map部分每读取一行数据都会执行一次，然后将分组后的结果交给Reduce处理。 比如输入文件是这样的： hadoop hive hive scala spark hadoop 那MapReduce程序Map端实现：将文字分割，然后形成格式的对。，都会调用一次。以上文件分割后形成： 然后Reduce端接收到的结果是按照上述的key进行分组后的结果，这里为什么Reduce端拿到的是上述结果分组后的结果呢？简单来说就是默认情况下，MapReduce已经指定了Map -> Reduce中间的分组和排序的规则。这个规则我们是可以自定义的，可以参考：二次排序。 最后Reducer会按照Reduce中指定具体的reduce规则，计算出每个单词出现的个数。即： ===> ===> ===> ===> 以上流程具体实现代码： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; public class WcDemo1 { //Map public static class MrMapper extends Mapper { private Text mapOutputKey = new Text(); private IntWritable mapOutputValue = new IntWritable(1); @Override //每读取一行，都会调用一次map() public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] strs = line.split(\" \"); for (String str : strs) { mapOutputKey.set(str); context.write(mapOutputKey, mapOutputValue); } } } //Reduce public static class MrReducer extends Reducer { private IntWritable outputV = new IntWritable(); @Override public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } outputV.set(sum); context.write(key, outputV); } } //Driver private int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException { //创建Job Configuration conf = new Configuration(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); //设置Job Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(MrMapper.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setReducerClass(MrReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //提交Job boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0 : 1; } public static void main(String[] args) throws InterruptedException, IOException, ClassNotFoundException { args = new String[] { \"hdfs://n1:8020/Test/input\", \"hdfs://n1:8020/Test/wcDemo1\" }; int status = new WcDemo1().run(args); System.out.println(status+\"--------------------\"); System.exit(status); } } 直接在Windows本地运行以上代码，就可以在HDFS Web UI界面看到运行后生成的结果。但是还有两个可以优化的地方。 首先，如果要将Java文件打包为jar包传到服务器上运行，遮掩过的方式显然不太合适。因为输入文件和输出文件的路径给死了，属于硬编码。优化代码见： 优化一 另外，以上只是以两行很少的单词作为输入文件举例。如果输入文件过大的话，在Map -> Reduce过程中的shuffle过程传输的数据量也会非常大，这种情况下就会占用大量的网络带宽。这时候我们可以通过用Map端的Combine（合并）来让Map端提前做一点合并，承担一点Reduce端的工作。优化方式见： 自定义Map端Combine规则 By renwujie            updated 2021-01-28 23:15:20 "},"03-code-demo/03-wd-count/01-wc-optimization/":{"url":"03-code-demo/03-wd-count/01-wc-optimization/","title":"01-wc-optimization","keywords":"","body":"WordCount案例代码优化一 接上节。针对硬编码问题，优化代码如下： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import java.io.IOException; public class WcDemo2 extends Configured implements Tool { //Map public static class MrMapper extends Mapper { private Text mapOutputKey = new Text(); private IntWritable mapOutputValue = new IntWritable(1); @Override //每读取一行，都会调用一次map() public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] strs = line.split(\" \"); for (String str : strs) { mapOutputKey.set(str); context.write(mapOutputKey, mapOutputValue); } } } //Reduce public static class MrReducer extends Reducer { private IntWritable outputV = new IntWritable(); @Override public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } outputV.set(sum); context.write(key, outputV); } } //Driver public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException { //创建Job Configuration conf = this.getConf(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); //设置Job Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(MrMapper.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setReducerClass(MrReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //提交Job boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0 : 1; } public static void main(String[] args) throws Exception { // args = new String[] { // \"hdfs://n1.com.rwj:8020/Test/input\", // \"hdfs://n1.com.rwj:8020/Test/wcDemo2\" // }; // int status = new WcDemo2().run(args); Configuration conf = new Configuration(); int status = ToolRunner.run(conf, new WcDemo2(), args); System.exit(status); } } By renwujie            updated 2021-01-28 22:13:22 "},"03-code-demo/03-wd-count/02-wc-custom-combine/":{"url":"03-code-demo/03-wd-count/02-wc-custom-combine/","title":"02-wc-custom-combine","keywords":"","body":"WordCount案例代码优化二 针对WordCount案例代码中提出的第二个可优化点，这里通过指定Map端的Combine来解决。 Combine是做什么的？ Combine类似于一个Reduce，只不过是在Map端执行的一个小的Reduce。 用于减少在Map -> Reduce中间传输的数据量。尽管Combine是可选的，但它有助于任务更有效的执行。 Combine是如何工作的？ Combine没有预定义的接口，但是它必需实现Reducer接口的reduce方法。 Combine会对Map端的每个输出结果进行操作，它必须有和Reducer类相同的输出：类型。 Combine可以从数据集上生成摘要信息，因为它代替了原始的Map输出。 代码案例 实现方式：自定义一个Combine类，实现Reducer接口并实现其reduce方法。具体代码如下： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; public class CombineDemo { public static class Map extends Mapper { private Text mapOutputKey = new Text(); private IntWritable moV = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] strs = line.split(\" \"); for (String str : strs) { mapOutputKey.set(str); context.write(mapOutputKey, moV); System.out.println(str + \" @@@ \" + moV.get()); } } } public static class Combiner extends Reducer { private IntWritable oV = new IntWritable(); @Override protected void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } oV.set(sum); context.write(key, oV); System.out.println(key + \" =!!!!= \" + oV.get()); } } public static class Reduce extends Reducer { private IntWritable oV = new IntWritable(); @Override protected void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } oV.set(sum); context.write(key, oV); System.out.println(key + \" ==== \" + oV.get()); } } private int run(String[] args) throws Exception{ Configuration conf = new Configuration(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(Map.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //shuffle job.setCombinerClass(Combiner.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0 : 1; } public static void main(String[] args) throws Exception { args = new String[] { // \"hdfs://n1.com.rwj:8020/Test/input\", // \"hdfs://n1.com.rwj:8020/Test/ComBT2\" // 本地测试可以用本地路径，方便查看结果 \"data/wc/wc.txt\", \"data/wc/output1\" }; int status = new CombineDemo().run(args); System.exit(status); } } By renwujie            updated 2021-02-04 21:47:13 "},"03-code-demo/04-second-sort/":{"url":"03-code-demo/04-second-sort/","title":"04-second-sort","keywords":"","body":"二次排序 默认情况下，在MapReduce程序中，Map输出的结果是按照默认的排序规则（字典序排序），只对对Key进行排序。因此Value的排序经常是不固定的。但是我们经常会遇到对Key和Value都需要排序的需求；比如Hadoop权威指南中求一年的最高气温，Key为年份，Value为最高气温；还有电商网站经常有按照天统计商品销售排行等需求。这些需求需要对Key和Value都进行排序，这时候就需要用到二次排序了。 二次排序原理 二次排序的核心是要对MapReduce的整个流程有很细致的了解。可以把二次排序分为以下几个阶段： Map起始阶段 在Map阶段，使用job.setInputFormatClass()定义的InputFormat，将输入的数据集分割(getSplits())成小数据块，同时InputFormat提供一个RecordReader的实现。在这里使用的是TextInputFormat，它提供的RecordReader会将文本的行号作为Key，这一行的文本作为Value。这就是指定Mapper的输入为的原因。然后调用自定义Mapper的map方法，将一个个键值对输入给Mapper的map方法。 Map最后阶段 在Map阶段的最后，会先调用job.setPartitionerClass()对这个Mapper的输出结果进行分区，每个分区映射到一个Reducer。每个分区内又会调用job.setSortComparatorClass()设置的类对Key进行排序。如果没有通过job.setSortComparatorClass()设置Key比较类，则使用Key类实现的compareTo方法。 Reduce阶段 在Reduce阶段，reduce()方法接收到所有映射到该Reduce的map的输出结果后，也会调用job.setSortComparatorClass()方法设置的Key比较器类C，对所有数据进行排序。然后开始构造一个Key对应的Value迭代器。这时就要用到分组，使用job.setGroupingComparatorClass方法设置分组类。只要分组类的Grouping规则比较两个Key是相同的，它们就属于同一组，它们的Value 就放在一个同一个迭代器中。而这个迭代器的Key使用属于同一个组的所有的Key的第一个。 最后进入到Reducer的reduce方法，reduce方法输入的是所有的Key及其对应的迭代器。注意输入与输出的类型必须与自定义的Reducer中声明的一致。 By renwujie            updated 2021-02-04 22:44:00 "},"03-code-demo/04-second-sort/01-demo1/":{"url":"03-code-demo/04-second-sort/01-demo1/","title":"01-demo1","keywords":"","body":" By renwujie            updated 2021-01-28 23:09:57 "},"03-code-demo/04-second-sort/02-demo2/":{"url":"03-code-demo/04-second-sort/02-demo2/","title":"02-demo2","keywords":"","body":" By renwujie            updated 2021-01-28 23:09:57 "},"03-code-demo/04-second-sort/03-demo3/":{"url":"03-code-demo/04-second-sort/03-demo3/","title":"03-demo3","keywords":"","body":" By renwujie            updated 2021-01-28 23:09:57 "},"03-code-demo/05-join/":{"url":"03-code-demo/05-join/","title":"05-join","keywords":"","body":"MapReduce实现SQL的Join功能 需求 案例需求是这个样子的：现有两个文件，一个文件是customer.csv——存储的是客户信息，另一个文件是order.csv——存储的是订单信息。内容分别如下（#后面的是注释内容）： customer.csv # id name phone 1,Stephanie Leung,555-555-5555 2,Edward Kim,123-456-7890 3,Jose Madriz,281-330-8004 4,David Stork,408-555-0000 orders.csv # id goods price date 3,Apple,12.95,02-Jun-2008 1,Banana,88.25,20-May-2008 2,Patato,32,30-Nov-2007 3,Orange,25.02,22-Jan-2009 请用MapReduce实现SQL的Join功能，最终形成以下结果： # id name phone goods price date 1,Stephanie Leung,555-555-5555,88.25,20-May-2008 2,Edward Kim,123-456-7890,32.00,30-Nov-2007 3,Jose Madriz,281-330-8004,25.02,22-Jan-2009 3,Jose Madriz,281-330-8004,12.95,02-Jun-2008 实现 具体思路：在Map端，将customer和order每一行的数据分割为的形式，这样，在数据从Map端流转到Reduce的时候，就形成了的形式。然后在Reduce端，将信息打平，形成多条customer和order的组合信息：、，即可。 这里需要我们自定义一个数据类型，来区分解析后得数据是客户（customer）信息，还是订单（order）信息。自定义数据类型需要实现Writable接口。 import org.apache.hadoop.io.Writable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.util.Objects; public class DataJoinWritable implements Writable { private String tag; private String data; public DataJoinWritable() { } private DataJoinWritable(String tag, String data) { this.set(tag, data); } public void set(String tag, String data) { this.setTag(tag); this.setData(data); } public String getTag() { return tag; } public void setTag(String tag) { this.tag = tag; } public String getData() { return data; } public void setData(String data) { this.data = data; } @Override public void write(DataOutput out) throws IOException { out.writeUTF(this.getTag()); out.writeUTF(this.getData()); } @Override public void readFields(DataInput in) throws IOException { this.setTag(in.readUTF()); this.setData(in.readUTF()); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; DataJoinWritable that = (DataJoinWritable) o; return Objects.equals(tag, that.tag) && Objects.equals(data, that.data); } @Override public int hashCode() { return Objects.hash(tag, data); } @Override public String toString() { return tag + \",\" + data; } } 然后实现MapRedue任务的代码： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import java.io.IOException; import java.util.ArrayList; import java.util.List; public class DataJoinMapReduce extends Configured implements Tool { public static class DataJoinMapper extends Mapper { private LongWritable mKey = new LongWritable(); private DataJoinWritable mVal = new DataJoinWritable(); @Override public void setup(Context context) throws IOException, InterruptedException { super.setup(context); } @Override protected void map(LongWritable key, Text line, Context context) throws IOException, InterruptedException { if(key.get() == 0) { return; } String[] values = line.toString().split(\",\"); if(3 != values.length && 4 != values.length) { return; } long cid = Long.parseLong(values[0]); if(3 == values.length) { String name = values[1]; String phone = values[2]; mKey.set(cid); mVal.set(\"customer\", name + \",\" + phone); // context.write(mKey, mVal); } if(4 == values.length) { String goods = values[1]; String price = values[2]; String date = values[3]; mKey.set(cid); mVal.set(\"order\", goods + \",\" + price + \",\" + date); // context.write(mKey, mVal); } System.out.println(mKey+\":\"+mVal); context.write(mKey, mVal); } @Override public void cleanup(Context context) throws IOException, InterruptedException { super.cleanup(context); } } public static class DataJoinReducer extends Reducer { private Text rVal = new Text(); @Override public void reduce(LongWritable key, Iterable values, Context context) throws IOException, InterruptedException { String customerInfo = \"\"; List orderList = new ArrayList<>(); for(DataJoinWritable val : values) { if(val.getTag().equals(\"customer\")) { customerInfo = val.getData(); } if(val.getTag().equals(\"order\")) { orderList.add(val.getData()); } } for(String orderInfo : orderList) { rVal.set(key.get() + \",\" + customerInfo + \",\" + orderInfo); context.write(NullWritable.get(), rVal); } } } @Override public int run(String[] args) throws Exception { Configuration conf = this.getConf(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(DataJoinMapReduce.class); Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outPath); job.setMapperClass(DataJoinMapper.class); job.setMapOutputKeyClass(LongWritable.class); job.setMapOutputValueClass(DataJoinWritable.class); job.setReducerClass(DataJoinReducer.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class); boolean status = job.waitForCompletion(true); return status ? 0 : 1; } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); args = new String[] { // \"hdfs://n3.com.rwj:8020/test/join/input\" , \"hdfs://n3.com.rwj:8020/test/join/output1\" \"data/join/input\", \"data/join/output\" }; int isSuccess = ToolRunner.run(conf, new DataJoinMapReduce(), args); System.exit(isSuccess); } } By renwujie            updated 2021-02-04 21:16:54 "},"03-code-demo/06-custom-counter/":{"url":"03-code-demo/06-custom-counter/","title":"06-custom-counter","keywords":"","body":" By renwujie            updated 2021-01-28 22:29:00 "},"04-architec/":{"url":"04-architec/","title":"04-architec","keywords":"","body":"架构 By renwujie            updated 2021-01-27 23:40:09 "},"05-src-code/":{"url":"05-src-code/","title":"05-src-code","keywords":"","body":"源码解读 By renwujie            updated 2021-01-27 23:24:50 "},"05-src-code/01-build/":{"url":"05-src-code/01-build/","title":"01-build","keywords":"","body":"简介 源码修改后要先编译，这里就以一个简单的例子去编译。代码修改给超链接到本章的具体页面中。 By renwujie            updated 2021-01-24 00:17:56 "}}