{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Apache Hadoop软件库是一个框架，该框架允许使用简单的编程模型跨计算机集群对大型数据集进行分布式处理。它旨在从单个服务器扩展到数千台机器，每台机器都提供本地的计算和存储。集群的每台计算机都可能会出现故障，所以该库本身不依赖于硬件来提供高可用性，而是被设计用来检测和处理应用程序层的故障，因此可以在计算机集群的顶部提供高可用性服务。 本文档旨在以简单粗报直接的方式帮助大家使用Hadoop。使用的版本是Hadoop2.7.X。 下载 单击此处下载Apache Hadoop™。 Hadoop的特性 可靠性，高容错 存储的可靠性 HDFS以块（block）的形式来存储数据，Hadoop 1.X版本中块大小是64MB；Hadoop 2.X版本中块的大小为128MB。用户可自定义块大小 HDFS分布式的存储文件块到集群的多台机器上，每台保存一个副本。用户可以自己设置副本数。 对每个存储块文件生成也给校验码，之后定期检测或读取到这个块的时候又会生成一个校验码进行匹配，没匹配上说明文件损坏，Hadoop会自动重新创建一个副本以保证数据的安全性。 计算的可靠性 可扩展性 集群扩容 损坏的硬件重新上架 By renwujie            updated 2021-01-23 23:18:27 "},"01-enviroment/":{"url":"01-enviroment/","title":"01-enviroment","keywords":"","body":"安装部署 单节点模式 Hadoop完全分布式集群 Hadoop HA集群 By renwujie            updated 2021-01-27 23:34:30 "},"01-enviroment/01-cluster-setup.html":{"url":"01-enviroment/01-cluster-setup.html","title":"01-cluster-setup.md","keywords":"","body":"部署完全分布式Hadoop集群 Purpose 本节描述如何搭建一个完全分布式的Hadoop集群，以3台服务器举例。可扩展到若干节点。本文档不涉及安全或高可用性等主题。 Prerequisites 参考：搭建高可用集群时Linux环境必需配置的先决条件，先将Linux基础环境搭建成功。 Installation 点击下载相应版本，本文档采用2.7.7。安装Hadoop集群需要解压所软件在所有的节点上，所以将硬件的功能进行划分很重要。 Component Planning 建议将NameNode和ResourceManager指定分开在两台节点上，这样即使某台节点挂了也不至于导致NameNode和ResoureManager同时挂掉。其余的节点则同时充当DataNode和NodeManager。通常建议将DataNode和ResourceManager安装在同一台节点上。 n1 n2 n3 NameNode/DataNode DataNode DataNode NodeManager NodeManager ResourceManager/NodeManager SecondaryNameNode HistoryServer Configure Hadoop HDFS的守护程序是NameNode，SecondaryNameNode和DataNode。 YARN守护程序是ResourceManager，NodeManager和WebAppProxy。如果要使用MapReduce，则MapReduce作业历史服务器也将运行。对于大型安装，它们通常在单独的主机上运行。 所有配置文件都在路径：$HADOOP_HOME/etc/hadoop 中。 配置JAVA_HOME变量 在hadoop-env.sh、yarn-env.sh、mapred-env.sh中指定JAVA_HOME的路径。 HDFS 配置hdfs文件系统的主机和端口，端口号在hadoop1.x版本默认使用的是9000，而在hadoop2.x中默认使用的是8020。 以及hadoop分布式集群经过格式化后数据保存的位置。 core-site.xml中配置： fs.defaultFS hdfs://n1:8020 hadoop.tmp.dir /opt/cluster/hadoop-2.7.7/data/tmp 在静态web页面上呈现内容时，以参数hadoop.http.staticuser.user指定的用户名作为筛选条件，默认dr.who。 core-site.xml中配置： hadoop.http.staticuser.user renwujie 测试环境可以关闭hdfs中文件访问权限，默认为true。 hdfs-site.xml中配置： dfs.permissions.enabled false YARN 配置ResourceManager的主机名；以及web访问地址。 yarn-site.xml中配置： yarn.resourcemanager.hostname n3 yarn.resourcemanager.webapp.address ${yarn.resourcemanager.hostname}:8088 配置执行MapReduce job的框架。可选值：local，classic，yarn。 mapred-site.xml 中配置： mapreduce.framework.name yarn 如果在mapred-site.xml中设置了mapreduce.framework.name为yarn的话，就需要配置yarn.nodemanager.aux-services参数了。该参数用于指定执行MapReduce job时，yarn使用的shuffle（混淆）技术。如果使用的话值为mapreduce_shuffle。 yarn-site.xml中配置： yarn.nodemanager.aux-services mapreduce_shuffle--> A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers SecondaryNameNode hdfs-site.xml中配置： dfs.namenode.secondary.http-address n1:50090 HistoryServer mapred-site.xml中配置： mapreduce.jobhistory.address n2:10020 mapreduce.jobhistory.webapp.address n2:19888 日志聚合 yarn-site.xml中配置： yarn.log-aggregation-enable true yarn.log-aggregation.retain-seconds 106800 Slaves文件 slaves文件用于指定DataNode和NodeManager所在节点，每行一个。 n1 n2 n3 Operating the Hadoop Cluster 完成以上配置后，将整个$HADOOP_HOME文件分发到集群中的所有节点上。分发前删除掉：$HADOOP_HOME/share/doc目录。因为该目录只是一些参考文档，且很大。分发时将占用大量带宽，浪费时间。 scp -r $HADOOP_HOME root@n2.com.rwj:/opt/ scp -r $HADOOP_HOME root@n3.com.rwj:/opt/ 格式化集群 要启动hadoop集群，需要将HDFS和YARN都启动。 首次启动HDFS时，需要将其格式化。在NameNode机器所在节点上执行： $ HADOOP_HOME/bin/hdfs namenode -format 启动/关闭集群 以下所有命令是启动命令，如果是关闭，将start替换为stop即可。启动时一边启动一边用命令jps查看相应进程是否启动成功。 [info]注意 以下命令单独启动某进程时，必需要在相应的节点上进行该命令的操作。 启动HDFS： # 使用 $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode $HADOOP_HOME/sbin/ hadoop-daemon.sh start datanode # 或者 $HADOOP_HOME/sbin/start-dfs.sh 启动YARN： # 使用 $HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager $HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager # 或者 $HADOOP_HOME/sbin/start-yarn.sh 启动SecondaryNamenode $HADOOP_HOME/sbin hadoop-daemon.sh start secondarynamenode 启动HistoryServer $HADOOP_HOME/bin/mapred --daemon start historyserver #或者 $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver 最终，对应于开始的组件规划（Component Planning）；每台节点上都应该看到相应的进程启动并稳定运行。 Web Interface 成功启动集群后即可查看对应组件的Web UI是否可以正常访问。 Daemon Web Interface Notes NameNode https://n1:50070 默认50070 ResourceManager https://n3:8088 默认8088 MapReduce JobHistory https://n2:19888 默认19888 补充 Hadoop的配置文件 每个模块对应一个配置文件，主要分两种：默认配置文件和自定义配置文件。 | 模块 | 默认配置文件 | 自定义配置文件 | | :-------- | :----------------- | :-------------- | | Core | core-default.xml | core-site.xml | | HDFS | hdfs-default.xml | hdfs-site.xml | | YARN | yarn-default.xml | yarn-site.xml | | MapReduce | mapred-default.xml | mapred-site.xml | 配置文件的优先级：Hadoop的守护进程在启动的时候会首先去读取默认配置文件，再去读取自定义配置文件。自定义配置文件的属性会覆盖掉默认配置文件。 By renwujie            updated 2021-01-27 23:37:06 "},"01-enviroment/02-cluster-ha.html":{"url":"01-enviroment/02-cluster-ha.html","title":"02-cluster-ha.md","keywords":"","body":"部署Hadoop HA集群 By renwujie            updated 2021-01-27 23:37:11 "},"02-basic-operate/":{"url":"02-basic-operate/","title":"02-basic-operate","keywords":"","body":"基本操作 By renwujie            updated 2021-01-27 23:22:29 "},"03-code-demo/":{"url":"03-code-demo/","title":"03-code-demo","keywords":"","body":"案例 代码案例在Windows环境中开发，需要WIndows本地做一些配置。参考： Windows本地开发Hadoop代码环境配置。 Hadoop编程主要时编写MapReduce任务。MapReduce任务模板参考： MapReduce 模板代码 案例参考： WordCount案例 MapReduce计数器使用 MapReduce Combine案例 二次排序案例 MapReduce实现join功能案例 以上所有案例可参考： By renwujie            updated 2021-01-28 14:52:32 "},"03-code-demo/01-dev-env/":{"url":"03-code-demo/01-dev-env/","title":"01-dev-env","keywords":"","body":"Windows本地开发Hadoop代码环境配置 安装好jdk、maven、idea等基础组件，具备Windows本地开发Java代码的环境； 下载Hadoop，并解压到本地； 下载相应版本的winutils.exe文件和hadoop.dll文件，放到Windows本地的$HADOOP_HOME/bin目录下； 在resource目录下添加我们安装好的集群中的：core-site.xml和hdfs-site.xml，两个配置文件； 创建Maven项目，在pom.xml中添加开发Hadoop代码所需依赖包。 org.apache.hadoop hadoop-client ${hadoop.version} 配置好以后的目录可参考： By renwujie            updated 2021-01-28 14:57:44 "},"03-code-demo/02-mr-model/":{"url":"03-code-demo/02-mr-model/","title":"02-mr-model","keywords":"","body":"MapReduce编程模板 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import java.io.IOException; /** * Hello world! */ public class MRModule extends Configured implements Tool { public static class MrMapper extends Mapper { @Override protected void setup(Context context) throws IOException, InterruptedException { super.setup(context); } @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { } @Override protected void cleanup(Context context) throws IOException, InterruptedException { super.cleanup(context); } } //Reduce public static class MrReducer extends Reducer { @Override public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { } } //Driver public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException { //创建Job // Configuration conf = new Configuration(); Configuration conf = this.getConf(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); //设置Job Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(MrMapper.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setReducerClass(MrReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //提交Job boolean status = job.waitForCompletion(true); return status ? 0 : 1; } public static void main(String[] args) throws Exception { // int status = new MRModule().run(args); Configuration conf = new Configuration(); int status = ToolRunner.run(conf, new MRModule(), args); System.exit(status); } } By renwujie            updated 2021-01-28 21:38:37 "},"03-code-demo/03-wd-count/":{"url":"03-code-demo/03-wd-count/","title":"03-wd-count","keywords":"","body":"WordCount案例 WordCount的流程是这样的：针对输入的文件，按照指定的分隔符将字段分割，并形成形式的键值对。Map部分每读取一行数据都会执行一次，然后将分组后的结果交给Reduce处理。 比如输入文件是这样的： hadoop hive hive scala spark hadoop 那MapReduce程序Map端实现：将文字分割，然后形成格式的对。，都会调用一次。以上文件分割后形成： 然后Reduce端接收到的结果是按照上述的key进行分组后的结果，这里为什么Reduce端拿到的是上述结果分组后的结果呢？简单来说就是默认情况下，MapReduce已经指定了Map -> Reduce中间的分组和排序的规则。这个规则我们是可以自定义的，可以参考：二次排序。 最后Reducer会按照Reduce中指定具体的reduce规则，计算出每个单词出现的个数。即： ===> ===> ===> ===> 以上流程具体实现代码： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; public class WcDemo1 { //Map public static class MrMapper extends Mapper { private Text mapOutputKey = new Text(); private IntWritable mapOutputValue = new IntWritable(1); @Override //每读取一行，都会调用一次map() public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] strs = line.split(\" \"); for (String str : strs) { mapOutputKey.set(str); context.write(mapOutputKey, mapOutputValue); } } } //Reduce public static class MrReducer extends Reducer { private IntWritable outputV = new IntWritable(); @Override public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } outputV.set(sum); context.write(key, outputV); } } //Driver private int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException { //创建Job Configuration conf = new Configuration(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); //设置Job Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(MrMapper.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setReducerClass(MrReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //提交Job boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0 : 1; } public static void main(String[] args) throws InterruptedException, IOException, ClassNotFoundException { args = new String[] { \"hdfs://n1:8020/Test/input\", \"hdfs://n1:8020/Test/wcDemo1\" }; int status = new WcDemo1().run(args); System.out.println(status+\"--------------------\"); System.exit(status); } } 直接在Windows本地运行以上代码，就可以在HDFS Web UI界面看到运行后生成的结果。但是还有两个可以优化的地方。 首先，如果要将Java文件打包为jar包传到服务器上运行，遮掩过的方式显然不太合适。因为输入文件和输出文件的路径给死了，属于硬编码。优化代码见： 优化一 另外，以上只是以两行很少的单词作为输入文件举例。如果输入文件过大的话，在Map -> Reduce过程中的shuffle过程传输的数据量也会非常大，这种情况下就会占用大量的网络带宽。这时候我们可以通过用Map端的Combine（合并）来让Map端提前做一点合并，承担一点Reduce端的工作。优化方式见： 自定义Map端Combine规则 By renwujie            updated 2021-01-28 23:15:20 "},"03-code-demo/03-wd-count/01-wc-optimization/":{"url":"03-code-demo/03-wd-count/01-wc-optimization/","title":"01-wc-optimization","keywords":"","body":"WordCount案例代码优化一 接上节。针对硬编码问题，优化代码如下： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import java.io.IOException; public class WcDemo2 extends Configured implements Tool { //Map public static class MrMapper extends Mapper { private Text mapOutputKey = new Text(); private IntWritable mapOutputValue = new IntWritable(1); @Override //每读取一行，都会调用一次map() public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] strs = line.split(\" \"); for (String str : strs) { mapOutputKey.set(str); context.write(mapOutputKey, mapOutputValue); } } } //Reduce public static class MrReducer extends Reducer { private IntWritable outputV = new IntWritable(); @Override public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } outputV.set(sum); context.write(key, outputV); } } //Driver public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException { //创建Job Configuration conf = this.getConf(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); //设置Job Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(MrMapper.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setReducerClass(MrReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //提交Job boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0 : 1; } public static void main(String[] args) throws Exception { // args = new String[] { // \"hdfs://n1.com.rwj:8020/Test/input\", // \"hdfs://n1.com.rwj:8020/Test/wcDemo2\" // }; // int status = new WcDemo2().run(args); Configuration conf = new Configuration(); int status = ToolRunner.run(conf, new WcDemo2(), args); System.exit(status); } } By renwujie            updated 2021-01-28 22:13:22 "},"03-code-demo/03-wd-count/02-wc-custom-combine/":{"url":"03-code-demo/03-wd-count/02-wc-custom-combine/","title":"02-wc-custom-combine","keywords":"","body":"WordCount案例代码优化二 针对WordCount案例代码中提出的第二个可优化点，这里通过指定Map端的Combine来解决。 Combine是做什么的？ Combine类似于一个Reduce，只不过是在Map端执行的一个小的Reduce。 用于减少在Map -> Reduce中间传输的数据量。尽管Combine是可选的，但它有助于任务更有效的执行。 Combine是如何工作的？ Combine没有预定义的接口，但是它必需实现Reducer接口的reduce方法。 Combine会对Map端的每个输出结果进行操作，它必须有和Reducer类相同的输出：类型。 Combine可以从数据集上生成摘要信息，因为它代替了原始的Map输出。 代码案例 实现方式：自定义一个Combine类，实现Reducer接口并实现其reduce方法。具体代码如下： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; public class CombineDemo { public static class Map extends Mapper { private Text mapOutputKey = new Text(); private IntWritable moV = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] strs = line.split(\" \"); for (String str : strs) { mapOutputKey.set(str); context.write(mapOutputKey, moV); System.out.println(str + \" @@@ \" + moV.get()); } } } public static class Combiner extends Reducer { private IntWritable oV = new IntWritable(); @Override protected void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } oV.set(sum); context.write(key, oV); System.out.println(key + \" =!!!!= \" + oV.get()); } } public static class Reduce extends Reducer { private IntWritable oV = new IntWritable(); @Override protected void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { int sum = 0; for (IntWritable v : values) { sum += v.get(); } oV.set(sum); context.write(key, oV); System.out.println(key + \" ==== \" + oV.get()); } } private int run(String[] args) throws Exception{ Configuration conf = new Configuration(); Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(this.getClass()); Path inputPath = new Path(args[0]); FileInputFormat.addInputPath(job, inputPath); Path outputPath = new Path(args[1]); FileOutputFormat.setOutputPath(job, outputPath); job.setMapperClass(Map.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //shuffle job.setCombinerClass(Combiner.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0 : 1; } public static void main(String[] args) throws Exception { args = new String[] { // \"hdfs://n1.com.rwj:8020/Test/input\", // \"hdfs://n1.com.rwj:8020/Test/ComBT2\" // 本地测试可以用本地路径，方便查看结果 \"data/wc/wc.txt\", \"data/wc/output1\" }; int status = new CombineDemo().run(args); System.exit(status); } } By renwujie            updated 2021-01-28 23:14:43 "},"03-code-demo/04-second-sort/":{"url":"03-code-demo/04-second-sort/","title":"04-second-sort","keywords":"","body":"二次排序 默认情况下，在MapReduce程序中，Map输出的结果是按照默认的排序规则（字典序排序）对Key进行排序了的。但是有时候默认的排序规则无法满足业务的具体要求，这时候就要用到二次排序了。 By renwujie            updated 2021-01-28 22:51:30 "},"03-code-demo/04-second-sort/01-demo1/":{"url":"03-code-demo/04-second-sort/01-demo1/","title":"01-demo1","keywords":"","body":" By renwujie            updated 2021-01-28 23:09:57 "},"03-code-demo/04-second-sort/02-demo2/":{"url":"03-code-demo/04-second-sort/02-demo2/","title":"02-demo2","keywords":"","body":" By renwujie            updated 2021-01-28 23:09:57 "},"03-code-demo/04-second-sort/03-demo3/":{"url":"03-code-demo/04-second-sort/03-demo3/","title":"03-demo3","keywords":"","body":" By renwujie            updated 2021-01-28 23:09:57 "},"03-code-demo/05-join/":{"url":"03-code-demo/05-join/","title":"05-join","keywords":"","body":" By renwujie            updated 2021-01-28 14:39:38 "},"03-code-demo/06-custom-counter/":{"url":"03-code-demo/06-custom-counter/","title":"06-custom-counter","keywords":"","body":" By renwujie            updated 2021-01-28 22:29:00 "},"04-architec/":{"url":"04-architec/","title":"04-architec","keywords":"","body":"架构 By renwujie            updated 2021-01-27 23:40:09 "},"05-src-code/":{"url":"05-src-code/","title":"05-src-code","keywords":"","body":"源码解读 By renwujie            updated 2021-01-27 23:24:50 "},"05-src-code/01-build/":{"url":"05-src-code/01-build/","title":"01-build","keywords":"","body":"简介 源码修改后要先编译，这里就以一个简单的例子去编译。代码修改给超链接到本章的具体页面中。 By renwujie            updated 2021-01-24 00:17:56 "}}